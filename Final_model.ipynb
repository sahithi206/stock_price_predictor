{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9764cd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (7960, 8, 8), Test shape: (1990, 8, 8)\n",
      "Epoch 1/20 | Loss: 0.120755\n",
      "Epoch 2/20 | Loss: 0.025985\n",
      "Epoch 3/20 | Loss: 0.017158\n",
      "Epoch 4/20 | Loss: 0.013801\n",
      "Epoch 5/20 | Loss: 0.011686\n",
      "Epoch 6/20 | Loss: 0.010528\n",
      "Epoch 7/20 | Loss: 0.009550\n",
      "Epoch 8/20 | Loss: 0.009175\n",
      "Epoch 9/20 | Loss: 0.008468\n",
      "Epoch 10/20 | Loss: 0.008095\n",
      "Epoch 11/20 | Loss: 0.007810\n",
      "Epoch 12/20 | Loss: 0.007515\n",
      "Epoch 13/20 | Loss: 0.007120\n",
      "Epoch 14/20 | Loss: 0.006812\n",
      "Epoch 15/20 | Loss: 0.006907\n",
      "Epoch 16/20 | Loss: 0.006593\n",
      "Epoch 17/20 | Loss: 0.006381\n",
      "Epoch 18/20 | Loss: 0.006196\n",
      "Epoch 19/20 | Loss: 0.006114\n",
      "Epoch 20/20 | Loss: 0.006035\n",
      "üìÅ Saved predictions for AAPL_stock_gdelt_final_merged ‚Üí predictions/AAPL_stock_gdelt_final_merged_predictions_20251022_203613.csv\n",
      "üìÅ Saved predictions for AMZN_stock_gdelt_final_merged ‚Üí predictions/AMZN_stock_gdelt_final_merged_predictions_20251022_203613.csv\n",
      "üìÅ Saved predictions for GOOG_stock_gdelt_final_merged ‚Üí predictions/GOOG_stock_gdelt_final_merged_predictions_20251022_203613.csv\n",
      "üìÅ Saved predictions for JPM_stock_gdelt_final_merged ‚Üí predictions/JPM_stock_gdelt_final_merged_predictions_20251022_203613.csv\n",
      "üìÅ Saved predictions for MSFT_stock_gdelt_final_merged ‚Üí predictions/MSFT_stock_gdelt_final_merged_predictions_20251022_203613.csv\n",
      "üìÅ Saved predictions for NVDA_stock_gdelt_final_merged ‚Üí predictions/NVDA_stock_gdelt_final_merged_predictions_20251022_203613.csv\n",
      "üìÅ Saved predictions for PFE_stock_gdelt_final_merged ‚Üí predictions/PFE_stock_gdelt_final_merged_predictions_20251022_203613.csv\n",
      "üìÅ Saved predictions for TSLA_stock_gdelt_final_merged ‚Üí predictions/TSLA_stock_gdelt_final_merged_predictions_20251022_203613.csv\n",
      "üìÅ Saved predictions for V_stock_gdelt_final_merged ‚Üí predictions/V_stock_gdelt_final_merged_predictions_20251022_203613.csv\n",
      "üìÅ Saved predictions for XOM_stock_gdelt_final_merged ‚Üí predictions/XOM_stock_gdelt_final_merged_predictions_20251022_203613.csv\n",
      "\n",
      "üìä Company metrics summary saved to: predictions/company_metrics_summary_20251022_203613.csv\n",
      "‚úÖ Model saved to models/transformer_stock_model_20251022_203613.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# =============================\n",
    "# 1Ô∏è‚É£ Load & preprocess dataset\n",
    "# =============================\n",
    "folder_path = \"processed_datasets\"  # CSV folder\n",
    "SEQ_LEN = 8  # number of past days per input\n",
    "TARGET_COL = \"Close\"\n",
    "FEATURE_COLS = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"EMA_7\", \"EMA_21\", \"sentiment_score\"]\n",
    "\n",
    "dfs = []\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".csv\"):\n",
    "        df = pd.read_csv(os.path.join(folder_path, file))\n",
    "        df[\"Company\"] = file.split(\".\")[0]\n",
    "        dfs.append(df)\n",
    "\n",
    "data = pd.concat(dfs, ignore_index=True)\n",
    "data = data.sort_values([\"Company\", \"Date\"]).reset_index(drop=True)\n",
    "data = data.dropna(subset=FEATURE_COLS + [TARGET_COL])\n",
    "\n",
    "# =============================\n",
    "# 2Ô∏è‚É£ Create sequences per company\n",
    "# =============================\n",
    "X_train_list, y_train_list = [], []\n",
    "X_test_list, y_test_list, companies_test_list = [], [], []\n",
    "\n",
    "for company in data[\"Company\"].unique():\n",
    "    df_c = data[data[\"Company\"] == company]\n",
    "    vals = df_c[FEATURE_COLS].values\n",
    "    target = df_c[TARGET_COL].values\n",
    "\n",
    "    X_c, y_c = [], []\n",
    "    for i in range(len(vals) - SEQ_LEN):\n",
    "        X_c.append(vals[i:i+SEQ_LEN])\n",
    "        y_c.append(target[i+SEQ_LEN])\n",
    "    \n",
    "    X_c = np.array(X_c, dtype=np.float32)\n",
    "    y_c = np.array(y_c, dtype=np.float32).reshape(-1,1)\n",
    "    \n",
    "    # Split per company\n",
    "    split_idx = int(0.8 * len(X_c))\n",
    "    X_train_list.append(X_c[:split_idx])\n",
    "    y_train_list.append(y_c[:split_idx])\n",
    "    \n",
    "    X_test_list.append(X_c[split_idx:])\n",
    "    y_test_list.append(y_c[split_idx:])\n",
    "    companies_test_list.extend([company] * (len(X_c) - split_idx))\n",
    "\n",
    "# Combine all companies\n",
    "X_train = np.concatenate(X_train_list, axis=0)\n",
    "y_train = np.concatenate(y_train_list, axis=0)\n",
    "X_test = np.concatenate(X_test_list, axis=0)\n",
    "y_test = np.concatenate(y_test_list, axis=0)\n",
    "companies_test = np.array(companies_test_list)\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "# =============================\n",
    "# 3Ô∏è‚É£ Normalize features\n",
    "# =============================\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "# Flatten for scaler\n",
    "X_flat = X_train.reshape(-1, X_train.shape[2])\n",
    "X_train_scaled = scaler_X.fit_transform(X_flat).reshape(X_train.shape)\n",
    "X_test_scaled = scaler_X.transform(X_test.reshape(-1, X_test.shape[2])).reshape(X_test.shape)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "y_test_scaled = scaler_y.transform(y_test)\n",
    "\n",
    "# =============================\n",
    "# 4Ô∏è‚É£ Create DataLoaders\n",
    "# =============================\n",
    "train_ds = TensorDataset(torch.tensor(X_train_scaled), torch.tensor(y_train_scaled))\n",
    "test_ds = TensorDataset(torch.tensor(X_test_scaled), torch.tensor(y_test_scaled))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "# =============================\n",
    "# 5Ô∏è‚É£ Transformer model\n",
    "# =============================\n",
    "class StockTransformer(nn.Module):\n",
    "    def __init__(self, feature_dim, seq_len, d_model=64, nhead=4, num_layers=2, dim_feedforward=128):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(feature_dim, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.regressor = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x[:, -1, :]  # Take last time step\n",
    "        return self.regressor(x)\n",
    "\n",
    "input_dim = X_train.shape[2]\n",
    "model = StockTransformer(feature_dim=input_dim, seq_len=SEQ_LEN)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# =============================\n",
    "# 6Ô∏è‚É£ Training setup\n",
    "# =============================\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "EPOCHS = 20\n",
    "\n",
    "# =============================\n",
    "# 7Ô∏è‚É£ Training loop\n",
    "# =============================\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        if torch.isnan(loss):\n",
    "            raise ValueError(\"NaN loss detected! Check your data.\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * xb.size(0)\n",
    "    epoch_loss /= len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {epoch_loss:.6f}\")\n",
    "\n",
    "# =============================\n",
    "# 8Ô∏è‚É£ Evaluation per company\n",
    "# =============================\n",
    "model.eval()\n",
    "preds, actuals = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (xb, yb) in enumerate(test_loader):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        out = model(xb)\n",
    "        preds.extend(out.squeeze().cpu().tolist())\n",
    "        actuals.extend(yb.squeeze().cpu().tolist())\n",
    "\n",
    "# Inverse transform\n",
    "preds = scaler_y.inverse_transform(np.array(preds).reshape(-1,1)).squeeze()\n",
    "actuals = scaler_y.inverse_transform(np.array(actuals).reshape(-1,1)).squeeze()\n",
    "\n",
    "# Create DataFrame\n",
    "test_df = pd.DataFrame({\n",
    "    \"Company\": companies_test,\n",
    "    \"Actual\": actuals,\n",
    "    \"Predicted\": preds,\n",
    "    \"Absolute_Error\": np.abs(actuals - preds)\n",
    "})\n",
    "\n",
    "# =============================\n",
    "# 9Ô∏è‚É£ Save per-company predictions & metrics\n",
    "# =============================\n",
    "os.makedirs(\"predictions\", exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "company_stats = []\n",
    "\n",
    "for company, grp in test_df.groupby(\"Company\"):\n",
    "    c_rmse = np.sqrt(mean_squared_error(grp[\"Actual\"], grp[\"Predicted\"]))\n",
    "    c_mae = mean_absolute_error(grp[\"Actual\"], grp[\"Predicted\"])\n",
    "    c_r2 = r2_score(grp[\"Actual\"], grp[\"Predicted\"])\n",
    "    \n",
    "    csv_path = f\"predictions/{company}_predictions_{timestamp}.csv\"\n",
    "    grp.to_csv(csv_path, index=False)\n",
    "    \n",
    "    company_stats.append({\n",
    "        \"Company\": company,\n",
    "        \"RMSE\": c_rmse,\n",
    "        \"MAE\": c_mae,\n",
    "        \"R2\": c_r2,\n",
    "        \"Records\": len(grp)\n",
    "    })\n",
    "    print(f\"üìÅ Saved predictions for {company} ‚Üí {csv_path}\")\n",
    "\n",
    "# Save summary metrics\n",
    "summary_df = pd.DataFrame(company_stats)\n",
    "summary_path = f\"predictions/company_metrics_summary_{timestamp}.csv\"\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "print(f\"\\nüìä Company metrics summary saved to: {summary_path}\")\n",
    "\n",
    "# =============================\n",
    "# üîü Save model\n",
    "# =============================\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "model_path = f\"models/transformer_stock_model_{timestamp}.pth\"\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'scaler_X': scaler_X,\n",
    "    'scaler_y': scaler_y\n",
    "}, model_path)\n",
    "\n",
    "print(f\"‚úÖ Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a629344",
   "metadata": {},
   "source": [
    "Miscellanous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae1e358",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\n",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1697\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:634\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1112\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1090\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:494\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2188\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n",
      "\u001b[1;32m   2185\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n",
      "\u001b[1;32m   2187\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n",
      "\u001b[0;32m-> 2188\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   2190\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   2192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n",
      "\u001b[1;32m   2193\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2257\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[0m\n",
      "\u001b[1;32m   2254\u001b[0m                 queue\u001b[38;5;241m.\u001b[39mput(internal_cmd)\n",
      "\u001b[1;32m   2255\u001b[0m                 wait_timeout \u001b[38;5;241m=\u001b[39m TIMEOUT_FAST\n",
      "\u001b[0;32m-> 2257\u001b[0m         \u001b[43mnotify_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   2258\u001b[0m         notify_event\u001b[38;5;241m.\u001b[39mclear()\n",
      "\u001b[1;32m   2260\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n",
      "\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n",
      "\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n",
      "\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n",
      "\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# =============================\n",
    "# Config\n",
    "# =============================\n",
    "RAW_FOLDER = \"processed_datasets\"       # Folder with your processed CSVs\n",
    "OUTPUT_FOLDER = \"processed_splits\"      # Where to save datasets\n",
    "SEQ_LEN = 5                             # Number of past days in a sequence\n",
    "BATCH_SIZE = 64\n",
    "TARGET_COL = \"Close\"                     # Or \"Daily_Return\" if you prefer\n",
    "\n",
    "FEATURE_COLS = [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\"EMA_7\",\"EMA_21\",\"sentiment_score\"]\n",
    "\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# =============================\n",
    "# 1Ô∏è‚É£ Load all CSVs\n",
    "# =============================\n",
    "dfs = []\n",
    "for file in os.listdir(RAW_FOLDER):\n",
    "    if file.endswith(\".csv\"):\n",
    "        df = pd.read_csv(os.path.join(RAW_FOLDER, file))\n",
    "        df[\"Company\"] = file.split(\".\")[0]\n",
    "        df = df.sort_values(\"Date\").reset_index(drop=True)\n",
    "        dfs.append(df)\n",
    "\n",
    "data = pd.concat(dfs, ignore_index=True)\n",
    "print(\"‚úÖ Combined dataset shape:\", data.shape)\n",
    "\n",
    "# =============================\n",
    "# 2Ô∏è‚É£ Create sequences per company\n",
    "# =============================\n",
    "X_seq_all, y_seq_all = [], []\n",
    "\n",
    "for company in data['Company'].unique():\n",
    "    df_c = data[data['Company']==company].sort_values(\"Date\")\n",
    "    df_c = df_c.dropna(subset=FEATURE_COLS + [TARGET_COL])\n",
    "    \n",
    "    X_c = df_c[FEATURE_COLS].values.astype(np.float32)\n",
    "    y_c = df_c[TARGET_COL].values.astype(np.float32).reshape(-1,1)\n",
    "    \n",
    "    for i in range(len(X_c) - SEQ_LEN):\n",
    "        X_seq_all.append(X_c[i:i+SEQ_LEN])\n",
    "        y_seq_all.append(y_c[i+SEQ_LEN])\n",
    "\n",
    "X_seq_all = np.array(X_seq_all)\n",
    "y_seq_all = np.array(y_seq_all)\n",
    "\n",
    "print(f\"‚úÖ Sequences created: X_seq_all={X_seq_all.shape}, y_seq_all={y_seq_all.shape}\")\n",
    "\n",
    "# =============================\n",
    "# 3Ô∏è‚É£ Shuffle sequences\n",
    "# =============================\n",
    "X_seq_all, y_seq_all = shuffle(X_seq_all, y_seq_all, random_state=42)\n",
    "print(\"‚úÖ Sequences shuffled\")\n",
    "\n",
    "# =============================\n",
    "# 4Ô∏è‚É£ Train-test split\n",
    "# =============================\n",
    "split_idx = int(0.8 * len(X_seq_all))\n",
    "X_train, y_train = X_seq_all[:split_idx], y_seq_all[:split_idx]\n",
    "X_test, y_test = X_seq_all[split_idx:], y_seq_all[split_idx:]\n",
    "\n",
    "# =============================\n",
    "# 5Ô∏è‚É£ Convert to PyTorch tensors & DataLoaders\n",
    "# =============================\n",
    "train_ds = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                         torch.tensor(y_train, dtype=torch.float32))\n",
    "test_ds = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                        torch.tensor(y_test, dtype=torch.float32))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"‚úÖ DataLoaders created successfully\")\n",
    "\n",
    "# =============================\n",
    "# 6Ô∏è‚É£ Save datasets\n",
    "# =============================\n",
    "torch.save(train_ds, os.path.join(OUTPUT_FOLDER, \"train_dataset.pt\"))\n",
    "torch.save(test_ds, os.path.join(OUTPUT_FOLDER, \"test_dataset.pt\"))\n",
    "\n",
    "# Optional CSV for inspection: flatten sequences\n",
    "train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "train_df = pd.DataFrame(train_flat, columns=[f\"{c}_t{i}\" for i in range(SEQ_LEN) for c in FEATURE_COLS])\n",
    "train_df[TARGET_COL] = y_train\n",
    "test_df = pd.DataFrame(test_flat, columns=[f\"{c}_t{i}\" for i in range(SEQ_LEN) for c in FEATURE_COLS])\n",
    "test_df[TARGET_COL] = y_test\n",
    "\n",
    "train_df.to_csv(os.path.join(OUTPUT_FOLDER, \"train_data.csv\"), index=False)\n",
    "test_df.to_csv(os.path.join(OUTPUT_FOLDER, \"test_data.csv\"), index=False)\n",
    "\n",
    "print(f\"üìÅ Saved datasets in '{OUTPUT_FOLDER}' folder:\")\n",
    "print(\" ‚îú‚îÄ‚îÄ train_dataset.pt\")\n",
    "print(\" ‚îú‚îÄ‚îÄ test_dataset.pt\")\n",
    "print(\" ‚îú‚îÄ‚îÄ train_data.csv\")\n",
    "print(\" ‚îî‚îÄ‚îÄ test_data.csv\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# =============================\n",
    "# 4Ô∏è‚É£ Model definition\n",
    "# =============================\n",
    "class StockSentimentModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "model = StockSentimentModel(input_dim=input_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch\n",
    "\n",
    "# =============================\n",
    "# 1Ô∏è‚É£ Run predictions\n",
    "# =============================\n",
    "model.eval()\n",
    "preds, actuals = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb_flat = xb.view(xb.size(0), -1)  # flatten before passing to model\n",
    "        out = model(xb_flat)\n",
    "        preds.extend(out.squeeze().tolist())\n",
    "        actuals.extend(yb.squeeze().tolist())\n",
    "\n",
    "preds = np.array(preds)\n",
    "actuals = np.array(actuals)\n",
    "\n",
    "# =============================\n",
    "# 2Ô∏è‚É£ Overall metrics\n",
    "# =============================\n",
    "rmse = np.sqrt(mean_squared_error(actuals, preds))\n",
    "mae = mean_absolute_error(actuals, preds)\n",
    "r2 = r2_score(actuals, preds)\n",
    "\n",
    "print(f\"\\n‚úÖ Overall RMSE: {rmse:.4f}\")\n",
    "print(f\"‚úÖ Overall MAE: {mae:.4f}\")\n",
    "print(f\"‚úÖ Overall R¬≤ Score: {r2:.4f}\")\n",
    "\n",
    "# =============================\n",
    "# 3Ô∏è‚É£ Prepare test DataFrame for per-company metrics\n",
    "# =============================\n",
    "# Create a record for each test sequence\n",
    "test_records = []\n",
    "\n",
    "# Optional: if you tracked 'Company' and 'Date' when creating sequences, you can fill them here\n",
    "for i in range(len(X_test)):\n",
    "    last_day_features = X_test[i, -1, :]  # last day in the sequence\n",
    "    # If you encoded Company as numeric, you can reverse map or just keep numeric\n",
    "    company_val = last_day_features[0] if last_day_features.shape[0] > 0 else \"Unknown\"\n",
    "    test_records.append({\n",
    "        \"Company\": company_val,\n",
    "        \"Date\": \"Unknown\",  # Replace with actual date if available\n",
    "        \"Actual\": actuals[i],\n",
    "        \"Predicted\": preds[i],\n",
    "        \"Absolute_Error\": abs(actuals[i] - preds[i])\n",
    "    })\n",
    "\n",
    "test_df = pd.DataFrame(test_records)\n",
    "\n",
    "# =============================\n",
    "# 4Ô∏è‚É£ Per-company metrics & CSV saving\n",
    "# =============================\n",
    "pred_dir = \"predictions\"\n",
    "os.makedirs(pred_dir, exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "company_stats = []\n",
    "for company, grp in test_df.groupby(\"Company\"):\n",
    "    c_rmse = np.sqrt(mean_squared_error(grp[\"Actual\"], grp[\"Predicted\"]))\n",
    "    c_mae = mean_absolute_error(grp[\"Actual\"], grp[\"Predicted\"])\n",
    "    c_r2 = r2_score(grp[\"Actual\"], grp[\"Predicted\"])\n",
    "\n",
    "    # Save CSV for this company\n",
    "    csv_path = os.path.join(pred_dir, f\"{company}_predictions_{timestamp}.csv\")\n",
    "    grp.to_csv(csv_path, index=False)\n",
    "\n",
    "    company_stats.append({\n",
    "        \"Company\": company,\n",
    "        \"RMSE\": c_rmse,\n",
    "        \"MAE\": c_mae,\n",
    "        \"R2\": c_r2,\n",
    "        \"Records\": len(grp)\n",
    "    })\n",
    "    print(f\"üìÅ Saved predictions for {company} ‚Üí {csv_path}\")\n",
    "\n",
    "# Save summary CSV\n",
    "summary_df = pd.DataFrame(company_stats)\n",
    "summary_path = os.path.join(pred_dir, f\"company_metrics_summary_{timestamp}.csv\")\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "print(f\"\\nüìä Per-company metrics saved to: {summary_path}\")\n",
    "\n",
    "# =============================\n",
    "# 5Ô∏è‚É£ Save the trained model\n",
    "# =============================\n",
    "model_dir = \"models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(model_dir, f\"stock_sentiment_model_{timestamp}.pth\")\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'rmse': rmse,\n",
    "    'mae': mae,\n",
    "    'r2': r2,\n",
    "    'input_dim': X_test.shape[2]  # number of features per day\n",
    "}, model_path)\n",
    "\n",
    "print(f\"‚úÖ Model saved successfully to: {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
