{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6fbe5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunkari/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from dateutil import parser\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import os, glob, pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18cb5d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers torch pandas tqdm python-dateutil joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d93c899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [04:23<00:00, 26.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved processed CSVs with neutral embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "RAW_FOLDER = \"/home/sunkari/Stock_price_predictor/Dataset\"\n",
    "OUTPUT_FOLDER = \"./processed_datasets\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"ProsusAI/finbert\",\n",
    "    use_safetensors=True\n",
    ").to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "def normalize_date_column(df):\n",
    "    def parse_date_safe(x):\n",
    "        try:\n",
    "            return parser.parse(str(x), dayfirst=False)\n",
    "        except:\n",
    "            return parser.parse(str(x), dayfirst=True)\n",
    "    df[\"Date\"] = df[\"Date\"].apply(parse_date_safe)\n",
    "    df = df.dropna(subset=[\"Date\"])\n",
    "    df = df.sort_values(\"Date\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def split_headlines(text):\n",
    "    if pd.isna(text): return []\n",
    "    return [t.strip() for t in str(text).split('|') if t.strip()]\n",
    "\n",
    "def get_emb(headlines):\n",
    "    \"\"\"\n",
    "    Compute mean embedding and sentiment score for a list of headlines.\n",
    "    Neutral if empty or NaN.\n",
    "    \"\"\"\n",
    "    if not headlines:\n",
    "        return np.zeros(768, dtype=np.float32), 0.0  # neutral embedding & score\n",
    "\n",
    "    embs, scores = [], []\n",
    "    for h in headlines:\n",
    "        if pd.isna(h) or h.strip() == \"\":\n",
    "            continue\n",
    "        inp = tokenizer(h, truncation=True, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            out = model(**inp)\n",
    "        cls = out.last_hidden_state[:, 0, :].mean(dim=0).cpu().numpy()\n",
    "        embs.append(cls)\n",
    "        scores.append(cls.mean())\n",
    "\n",
    "    if len(embs) == 0:\n",
    "        return np.zeros(768, dtype=np.float32), 0.0\n",
    "\n",
    "    return np.mean(embs, axis=0), np.mean(scores)\n",
    "\n",
    "\n",
    "files = [f for f in os.listdir(RAW_FOLDER) if f.endswith(\".csv\")]\n",
    "for f in tqdm(files):\n",
    "    df = pd.read_csv(os.path.join(RAW_FOLDER, f))\n",
    "    df.columns = df.columns.str.strip().str.replace('\\ufeff','')  # clean column names\n",
    "    df = normalize_date_column(df)\n",
    "    df[\"Headline_List\"] = df[\"Headlines\"].apply(split_headlines)\n",
    "\n",
    "    embs, scores = [], []\n",
    "    for hl in tqdm(df[\"Headline_List\"], leave=False):\n",
    "        e, s = get_emb(hl)\n",
    "        embs.append(e)\n",
    "        scores.append(s)\n",
    "\n",
    "    emb_df = pd.DataFrame(embs, columns=[f\"emb_{i}\" for i in range(768)])\n",
    "    df[\"sentiment_score\"] = scores\n",
    "    df_out = pd.concat([df.drop(columns=[\"Headline_List\"]), emb_df], axis=1)\n",
    "    df_out.to_csv(os.path.join(OUTPUT_FOLDER, f.replace(\".csv\", \"_merged.csv\")), index=False)\n",
    "print(\"✅ Saved processed CSVs with neutral embeddings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d4584fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved train/test windows in ./windows\n"
     ]
    }
   ],
   "source": [
    "import os, glob, pickle, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "WINDOW_SIZE = 8\n",
    "INPUT_FOLDER = \"./processed_datasets\"\n",
    "SAVE_FOLDER = \"./windows\"\n",
    "os.makedirs(SAVE_FOLDER, exist_ok=True)\n",
    "\n",
    "files = glob.glob(os.path.join(INPUT_FOLDER, \"*_merged.csv\"))\n",
    "all_windows, companies = [], []\n",
    "\n",
    "for f in tqdm(files):\n",
    "    df = pd.read_csv(f, parse_dates=[\"Date\"])\n",
    "    ticker = df[\"Ticker\"].iloc[0] if \"Ticker\" in df.columns else os.path.basename(f).split(\"_\")[0]\n",
    "    \n",
    "    # ✅ Keep only numeric columns (drops text like Headlines, Company, etc.)\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    feat_cols = numeric_df.columns.tolist()\n",
    "    \n",
    "    # Ensure Close exists for target y\n",
    "    if \"Close\" not in df.columns:\n",
    "        print(f\"⚠️ Skipping {f}: 'Close' column missing.\")\n",
    "        continue\n",
    "\n",
    "    for i in range(len(df) - WINDOW_SIZE):\n",
    "        X = numeric_df.iloc[i:i+WINDOW_SIZE].values.astype(np.float32)\n",
    "        y = float(df[\"Close\"].iloc[i+WINDOW_SIZE])\n",
    "        all_windows.append((X, y, ticker))\n",
    "\n",
    "    companies.append(ticker)\n",
    "\n",
    "random.shuffle(all_windows)\n",
    "split = int(0.8 * len(all_windows))\n",
    "train_windows = all_windows[:split]\n",
    "test_windows = all_windows[split:]\n",
    "\n",
    "pickle.dump(train_windows, open(f\"{SAVE_FOLDER}/train_windows.pkl\", \"wb\"))\n",
    "pickle.dump(test_windows, open(f\"{SAVE_FOLDER}/test_windows.pkl\", \"wb\"))\n",
    "pickle.dump(sorted(list(set(companies))), open(f\"{SAVE_FOLDER}/company_list.pkl\", \"wb\"))\n",
    "print(\"✅ Saved train/test windows in\", SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59508b8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
