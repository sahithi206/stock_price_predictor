{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c493d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunkari/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80e51172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a64a1bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Loading model: yiyanghkust/finbert-tone\n",
      "‚úÖ Model loaded successfully.\n",
      "\n",
      "üîç Processing XOM_stock_gdelt_final.csv ...\n",
      "üìÇ Loaded file with 1003 rows.\n",
      "üóìÔ∏è Normalized dates, 1003 rows remain after cleaning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentiment XOM_stock_gdelt_final.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1003/1003 [03:05<00:00,  5.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved train/test split for XOM_stock_gdelt_final.csv | Train=802, Test=201 | ‚è±Ô∏è 185.98s\n",
      "\n",
      "üîç Processing MSFT_stock_gdelt_final.csv ...\n",
      "üìÇ Loaded file with 1003 rows.\n",
      "üóìÔ∏è Normalized dates, 1003 rows remain after cleaning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentiment MSFT_stock_gdelt_final.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1003/1003 [03:40<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved train/test split for MSFT_stock_gdelt_final.csv | Train=802, Test=201 | ‚è±Ô∏è 220.27s\n",
      "\n",
      "üîç Processing V_stock_gdelt_final.csv ...\n",
      "üìÇ Loaded file with 1003 rows.\n",
      "üóìÔ∏è Normalized dates, 1003 rows remain after cleaning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentiment V_stock_gdelt_final.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1003/1003 [02:36<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved train/test split for V_stock_gdelt_final.csv | Train=802, Test=201 | ‚è±Ô∏è 156.33s\n",
      "\n",
      "üîç Processing PFE_stock_gdelt_final.csv ...\n",
      "üìÇ Loaded file with 1003 rows.\n",
      "üóìÔ∏è Normalized dates, 1003 rows remain after cleaning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentiment PFE_stock_gdelt_final.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1003/1003 [04:06<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved train/test split for PFE_stock_gdelt_final.csv | Train=802, Test=201 | ‚è±Ô∏è 246.90s\n",
      "\n",
      "üîç Processing NVDA_stock_gdelt_final.csv ...\n",
      "üìÇ Loaded file with 1003 rows.\n",
      "üóìÔ∏è Normalized dates, 1003 rows remain after cleaning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentiment NVDA_stock_gdelt_final.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1003/1003 [03:21<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved train/test split for NVDA_stock_gdelt_final.csv | Train=802, Test=201 | ‚è±Ô∏è 201.23s\n",
      "\n",
      "üîç Processing AMZN_stock_gdelt_final.csv ...\n",
      "üìÇ Loaded file with 1003 rows.\n",
      "üóìÔ∏è Normalized dates, 1003 rows remain after cleaning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentiment AMZN_stock_gdelt_final.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1003/1003 [02:48<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved train/test split for AMZN_stock_gdelt_final.csv | Train=802, Test=201 | ‚è±Ô∏è 168.71s\n",
      "\n",
      "üîç Processing GOOG_stock_gdelt_final.csv ...\n",
      "üìÇ Loaded file with 1003 rows.\n",
      "üóìÔ∏è Normalized dates, 1003 rows remain after cleaning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentiment GOOG_stock_gdelt_final.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1003/1003 [03:42<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved train/test split for GOOG_stock_gdelt_final.csv | Train=802, Test=201 | ‚è±Ô∏è 222.84s\n",
      "\n",
      "üîç Processing TSLA_stock_gdelt_final.csv ...\n",
      "üìÇ Loaded file with 1003 rows.\n",
      "üóìÔ∏è Normalized dates, 1003 rows remain after cleaning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentiment TSLA_stock_gdelt_final.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1003/1003 [04:32<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved train/test split for TSLA_stock_gdelt_final.csv | Train=802, Test=201 | ‚è±Ô∏è 273.02s\n",
      "\n",
      "üîç Processing JPM_stock_gdelt_final.csv ...\n",
      "üìÇ Loaded file with 1003 rows.\n",
      "üóìÔ∏è Normalized dates, 1003 rows remain after cleaning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentiment JPM_stock_gdelt_final.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1003/1003 [03:39<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved train/test split for JPM_stock_gdelt_final.csv | Train=802, Test=201 | ‚è±Ô∏è 219.45s\n",
      "\n",
      "üîç Processing AAPL_stock_gdelt_final.csv ...\n",
      "üìÇ Loaded file with 1003 rows.\n",
      "üóìÔ∏è Normalized dates, 1003 rows remain after cleaning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentiment AAPL_stock_gdelt_final.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1003/1003 [03:02<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved train/test split for AAPL_stock_gdelt_final.csv | Train=802, Test=201 | ‚è±Ô∏è 182.31s\n",
      "\n",
      "üèÅ All files processed successfully in 34.62 minutes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from dateutil import parser\n",
    "\n",
    "# =====================================================\n",
    "LOG_FILE = \"processing_log.txt\"\n",
    "logging.basicConfig(\n",
    "    filename=LOG_FILE,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "def log(msg, level=\"info\"):\n",
    "    tqdm.write(msg)\n",
    "    if level == \"error\":\n",
    "        logging.error(msg)\n",
    "    elif level == \"warning\":\n",
    "        logging.warning(msg)\n",
    "    else:\n",
    "        logging.info(msg)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "def normalize_date_column(df):\n",
    "    def parse_date_safe(x):\n",
    "        try:\n",
    "            return parser.parse(str(x), dayfirst=False)\n",
    "        except Exception:\n",
    "            try:\n",
    "                return parser.parse(str(x), dayfirst=True)\n",
    "            except Exception:\n",
    "                return None\n",
    "\n",
    "    df[\"Date\"] = df[\"Date\"].apply(parse_date_safe)\n",
    "    df = df.dropna(subset=[\"Date\"])\n",
    "    df = df.sort_values(\"Date\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_headlines(text):\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    return [t.strip() for t in str(text).split('|') if t.strip()]\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "DATA_DIR = \"/home/sunkari/Stock_price_predictor/Dataset\"\n",
    "OUTPUT_DIR = \"./Processed\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_RATIO = 0.8  # 80% train, 20% test\n",
    "\n",
    "# =====================================================\n",
    "MODEL_NAME = \"yiyanghkust/finbert-tone\"\n",
    "log(f\"üîπ Loading model: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "log(\"‚úÖ Model loaded successfully.\")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "def get_sentiment_scores(texts):\n",
    "    if len(texts) == 0:\n",
    "        return [0.0, 1.0, 0.0]  # Neutral default\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = F.softmax(outputs.logits, dim=-1)\n",
    "    return probs.mean(dim=0).numpy().tolist()\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "start_time_total = time.time()\n",
    "\n",
    "for file in os.listdir(DATA_DIR):\n",
    "    if not file.endswith(\".csv\"):\n",
    "        continue\n",
    "    start_time = time.time()\n",
    "    company_path = os.path.join(DATA_DIR, file)\n",
    "    log(f\"\\nüîç Processing {file} ...\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(company_path)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        log(f\"üìÇ Loaded file with {len(df)} rows.\")\n",
    "\n",
    "        df = normalize_date_column(df)\n",
    "        log(f\"üóìÔ∏è Normalized dates, {len(df)} rows remain after cleaning.\")\n",
    "\n",
    "        # Split headlines into lists\n",
    "        df[\"Headline_List\"] = df[\"Headlines\"].apply(split_headlines)\n",
    "\n",
    "        # Compute daily sentiment\n",
    "        sentiments = []\n",
    "        for headlines in tqdm(df[\"Headline_List\"].tolist(), desc=f\"Sentiment {file}\"):\n",
    "            try:\n",
    "                probs = get_sentiment_scores(headlines)\n",
    "                sentiments.append(probs)\n",
    "            except Exception as e:\n",
    "                log(f\"‚ö†Ô∏è Error processing headlines: {headlines[:3]}... | {e}\", \"warning\")\n",
    "                sentiments.append([0.0, 1.0, 0.0])  # default neutral\n",
    "\n",
    "        sentiments = pd.DataFrame(sentiments, columns=[\"negative\", \"neutral\", \"positive\"])\n",
    "        df = pd.concat([df, sentiments], axis=1)\n",
    "\n",
    "        # =====================================================\n",
    "        # ‚úÇÔ∏è Split into Train and Test\n",
    "        # =====================================================\n",
    "        split_idx = int(len(df) * TRAIN_RATIO)\n",
    "        train_df = df.iloc[:split_idx].reset_index(drop=True)\n",
    "        test_df = df.iloc[split_idx:].reset_index(drop=True)\n",
    "\n",
    "        # Save processed versions\n",
    "        base_name = os.path.splitext(file)[0]\n",
    "        train_out = os.path.join(OUTPUT_DIR, f\"{base_name}_train.csv\")\n",
    "        test_out = os.path.join(OUTPUT_DIR, f\"{base_name}_test.csv\")\n",
    "\n",
    "        train_df.to_csv(train_out, index=False)\n",
    "        test_df.to_csv(test_out, index=False)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        log(f\"‚úÖ Saved train/test split for {file} | Train={len(train_df)}, Test={len(test_df)} | ‚è±Ô∏è {elapsed:.2f}s\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"‚ùå Error processing {file}: {e}\", \"error\")\n",
    "\n",
    "total_time = time.time() - start_time_total\n",
    "log(f\"\\nüèÅ All files processed successfully in {total_time/60:.2f} minutes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
